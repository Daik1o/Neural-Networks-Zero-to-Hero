{"cells":[{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":584,"status":"ok","timestamp":1728066751276,"user":{"displayName":"Daiki","userId":"04202454946609493394"},"user_tz":180},"id":"Ne9kN2646KmR"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1728066751823,"user":{"displayName":"Daiki","userId":"04202454946609493394"},"user_tz":180},"id":"HOcorH8M6cro","outputId":"b6af4484-2a63-4aa4-805d-0f50311450da"},"outputs":[{"name":"stdout","output_type":"stream","text":["first 10 words['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n","length of words: 32033\n","min word length 2 and max word length 15\n"]}],"source":["# reading the data\n","words = open(\"names.txt\", \"r\").read().splitlines()\n","\n","# Exploring\n","print(f\"first 10 words{words[:10]}\")\n","print(f\"length of words: {len(words)}\")\n","print(f\"min word length {min(len(w) for (w) in words)} and max word length {max(len(w) for (w) in words)}\")"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1728066751824,"user":{"displayName":"Daiki","userId":"04202454946609493394"},"user_tz":180},"id":"UNUgfEOF-jsN","outputId":"c691fb87-ca66-49fa-ed74-7f997bc25b1f"},"outputs":[{"data":{"text/plain":["['emma',\n"," 'olivia',\n"," 'ava',\n"," 'isabella',\n"," 'sophia',\n"," 'charlotte',\n"," 'mia',\n"," 'amelia',\n"," 'harper',\n"," 'evelyn']"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["words[:10]"]},{"cell_type":"markdown","metadata":{"id":"FaIeMTsC8uhK"},"source":["## E01: Train a Trigram Language Model\n"," Train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1728066751824,"user":{"displayName":"Daiki","userId":"04202454946609493394"},"user_tz":180},"id":"efMTi-LT9-vL"},"outputs":[],"source":["# make a list of characters (a -\u003e z)\n","chars = sorted(list(set(\"\".join(words))))\n","chars = [\".\"] + chars\n","\n","# # make a dictionary of character to index\n","stoi = {ch: i for (i, ch) in enumerate(chars)}\n","\n","# # make a dictionary of index to character\n","itos = {i: ch for (ch, i) in stoi.items()}"]},{"cell_type":"markdown","metadata":{"id":"V6Xri4Zb9yqn"},"source":["### 1 - Using Counting"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1728066751824,"user":{"displayName":"Daiki","userId":"04202454946609493394"},"user_tz":180},"id":"qyfGTMWtuN3S"},"outputs":[],"source":["N = torch.ones((27,27,27), dtype=torch.int32)"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1728066751824,"user":{"displayName":"Daiki","userId":"04202454946609493394"},"user_tz":180},"id":"IikKBFyz0BtO"},"outputs":[],"source":["# 27 27 27\n","# 27 27 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IWQnGhbEpp3t"},"outputs":[],"source":["N[0,0,0] = 0\n","for w in words:\n","  chs = ['.'] + list(w) + ['.']\n","  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n","    ix1 = stoi[ch1]\n","    ix2 = stoi[ch2]\n","    ix3 = stoi[ch3]\n","    N[ix1,ix2,ix3] += 1\n","\n","P = N.float()\n","P /= P.sum(2, keepdim=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PcNJP0Lb1YFQ"},"outputs":[{"data":{"text/plain":["1.0"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["P[10,4].sum().item()"]},{"cell_type":"markdown","metadata":{"id":"iBa4bvVZ3QvO"},"source":["#### Loss Function"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ecHvWcUl3Vl_"},"outputs":[{"name":"stdout","output_type":"stream","text":["log_likelihood=tensor(-410414.9688)\n","nll=tensor(410414.9688)\n","2.092747449874878\n"]}],"source":["log_likelihood = 0.0\n","n = 0\n","\n","for w in words:\n","  chs = ['.'] + list(w) + ['.']\n","  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n","    ix1 = stoi[ch1]\n","    ix2 = stoi[ch2]\n","    ix3 = stoi[ch3]\n","    prob = P[ix1, ix2, ix3]\n","    logprob = torch.log(prob)\n","    log_likelihood += logprob\n","    n += 1\n","    # print(f'{ch1}{ch2}{ch3}: {logprob:.4f}')\n","\n","print(f'{log_likelihood=}')\n","nll = -log_likelihood\n","print(f'{nll=}')\n","print(f'{nll/n}')"]},{"cell_type":"markdown","metadata":{"id":"5h75QJ4q0dyk"},"source":["#### Sampling"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_UgJMApAlY2N"},"outputs":[{"data":{"text/plain":["['junide.', 'ilyasid.', 'prelay.', 'ocin.', 'fairritoper.']"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["g = torch.Generator().manual_seed((2147483647))\n","\n","names = []\n","for i in range(5):\n","  out = []\n","  ix1, ix2 = 0, 0\n","  while True:\n","    p = P[ix1, ix2]\n","    ix1 = ix2\n","    ix2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() # this tell us what index will be next\n","    out.append(itos[ix2])\n","    if ix2 == 0:\n","      break\n","  names.append(\"\".join(out))\n","\n","names"]},{"cell_type":"markdown","metadata":{"id":"HX7eT92391u8"},"source":["### 2 - Using MLP"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"xxJlldZI8uQe"},"outputs":[],"source":["# Prepare the dataset\n","xs, ys = [], []\n","num = 0\n","\n","for w in words:\n","  chs = ['.'] + list(w) + ['.']\n","  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n","    ix1 = stoi[ch1]\n","    ix2 = stoi[ch2]\n","    ix3 = stoi[ch3]\n","\n","    xs.append([ix1, ix2])\n","    ys.append(ix3)\n","\n","    num += 1\n","\n","xs = torch.tensor(xs, dtype=torch.int64)\n","ys = torch.tensor(ys, dtype=torch.int64)\n","\n","# initialize the network\n","g = torch.Generator().manual_seed(2147483647)\n","W1 = torch.randn((27*2, 27), generator=g, requires_grad=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-1_pxJ6XIe1X"},"outputs":[{"name":"stdout","output_type":"stream","text":["0: 4.1863\n","10: 2.5042\n","20: 2.3797\n","30: 2.3327\n","40: 2.3081\n","50: 2.2932\n","60: 2.2833\n","70: 2.2762\n","80: 2.2709\n","90: 2.2668\n","99: 2.2637\n"]}],"source":["# gradient descent\n","rep = 100\n","\n","for k in range(rep):\n","\n","  # forward pass\n","  xenc = F.one_hot(xs, num_classes=27).float()\n","  xenc = xenc.view(-1, 27*2) # this transforms the matrix from [N, 2, 27] -\u003e [N, 27*2] | (N, 27*2) @ (27*2, 27) -\u003e (N,   27)\n","\n","  # softmax\n","  logits = xenc @ W1 # log-counts\n","  counts = logits.exp() # equivalent N\n","  probs = counts / counts.sum(1, keepdims=True)\n","  # probs.shape = [4, 27]. Isso significa que, para cada \"trigram\" (que são 4 no total), ele output probabilidades\n","\n","  # loss\n","  loss = -probs[torch.arange(num), ys].log().mean()\n","\n","  if k % 10 == 0:\n","      print(f\"{k}: {loss.item():.4f}\")\n","\n","\n","  # backward pass\n","  W1.grad = None # set to zero the gradient\n","  loss.backward()\n","\n","\n","  # update\n","  W1.data += -50 * W1.grad\n","\n","  if k == rep-1:\n","      print(f\"{k}: {loss.item():.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qer2kmseADsZ"},"outputs":[{"name":"stdout","output_type":"stream","text":["--------\n","trigram example 1: .em (indexes 0,5,13)\n","input to the neural net: 0 5\n","label (actual next character): 13\n","output probabilities from the neural net: tensor([2.2573e-03, 7.4474e-02, 6.3476e-03, 3.8791e-03, 3.1316e-02, 6.3107e-02,\n","        1.3237e-03, 1.7223e-03, 1.0195e-02, 3.6011e-02, 1.5039e-03, 4.0430e-03,\n","        2.4968e-01, 6.8560e-02, 6.9603e-02, 2.6674e-02, 1.2084e-03, 2.1775e-04,\n","        1.6794e-01, 3.8515e-02, 1.6396e-02, 1.3999e-02, 4.5653e-02, 1.8738e-03,\n","        1.3072e-03, 4.6026e-02, 1.6177e-02], grad_fn=\u003cSelectBackward0\u003e)\n","probability assigned by the net to the correct character: 0.0685596615076065\n","log likelihood: -2.680050849914551\n","negative log likelihood: 2.680050849914551\n","--------\n","trigram example 2: emm (indexes 5,13,13)\n","input to the neural net: 5 13\n","label (actual next character): 13\n","output probabilities from the neural net: tensor([0.2328, 0.2555, 0.0166, 0.0109, 0.0055, 0.0825, 0.0025, 0.0069, 0.0056,\n","        0.1645, 0.0046, 0.0059, 0.0066, 0.0475, 0.0023, 0.0478, 0.0054, 0.0011,\n","        0.0166, 0.0085, 0.0079, 0.0092, 0.0037, 0.0005, 0.0010, 0.0393, 0.0086],\n","       grad_fn=\u003cSelectBackward0\u003e)\n","probability assigned by the net to the correct character: 0.04748281463980675\n","log likelihood: -3.0473873615264893\n","negative log likelihood: 3.0473873615264893\n","--------\n","trigram example 3: mma (indexes 13,13,1)\n","input to the neural net: 13 13\n","label (actual next character): 1\n","output probabilities from the neural net: tensor([0.0256, 0.2775, 0.0081, 0.0159, 0.0029, 0.2123, 0.0020, 0.0010, 0.0026,\n","        0.2740, 0.0024, 0.0136, 0.0034, 0.0041, 0.0011, 0.0242, 0.0051, 0.0022,\n","        0.0326, 0.0023, 0.0036, 0.0078, 0.0011, 0.0011, 0.0042, 0.0672, 0.0023],\n","       grad_fn=\u003cSelectBackward0\u003e)\n","probability assigned by the net to the correct character: 0.2775304317474365\n","log likelihood: -1.2818247079849243\n","negative log likelihood: 1.2818247079849243\n","--------\n","trigram example 4: ma. (indexes 13,1,0)\n","input to the neural net: 13 1\n","label (actual next character): 0\n","output probabilities from the neural net: tensor([0.0889, 0.0099, 0.0100, 0.0249, 0.0401, 0.0213, 0.0033, 0.0049, 0.0508,\n","        0.0486, 0.0037, 0.0451, 0.1048, 0.0116, 0.1386, 0.0007, 0.0021, 0.0011,\n","        0.2400, 0.0243, 0.0307, 0.0049, 0.0083, 0.0027, 0.0101, 0.0612, 0.0073],\n","       grad_fn=\u003cSelectBackward0\u003e)\n","probability assigned by the net to the correct character: 0.08886528760194778\n","log likelihood: -2.4206337928771973\n","negative log likelihood: 2.4206337928771973\n","========\n","average negative log likelihood, i.e. loss = 2.3574740886688232\n"]}],"source":["nlls = torch.zeros(4)\n","for i in range(4):\n","    # i-th trigram:\n","    x1 = xs[i][0].item()  # 1st input character index\n","    x2 = xs[i][1].item()  # 2nd input character index\n","\n","    y = ys[i].item()  # label character index\n","    print('--------')\n","    print(f'trigram example {i+1}: {itos[x1]}{itos[x2]}{itos[y]} (indexes {x1},{x2},{y})')\n","    print('input to the neural net:', x1, x2)\n","    print('label (actual next character):', y)\n","    print('output probabilities from the neural net:', probs[i])\n","    p = probs[i, y]\n","    print('probability assigned by the net to the correct character:', p.item())\n","    logp = torch.log(p)\n","    print('log likelihood:', logp.item())\n","    nll = -logp\n","    print('negative log likelihood:', nll.item())\n","    nlls[i] = nll\n","\n","print('========')\n","print('average negative log likelihood, i.e. loss =', nlls.mean().item())\n"]},{"cell_type":"markdown","metadata":{"id":"ZZK0QhSj8MFQ"},"source":["#### Sampling"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Wgw19o1B8NS-"},"outputs":[{"name":"stdout","output_type":"stream","text":["aunide.\n","aliasad.\n","ushfay.\n","ainn.\n","aui.\n"]}],"source":["# finally, sample from the 'neural net' model\n","g = torch.Generator().manual_seed(2147483647)\n","\n","for i in range(5):\n","    out = []\n","    ix1, ix2 = 0, 0\n","    while True:\n","        # ---------------\n","        # # BEFORE:\n","        # # p = P[ix]\n","        # ---------------\n","        # NOW:\n","        xenc = F.one_hot(torch.tensor([ix1,ix2]), num_classes=27).float()\n","        xenc = xenc.view(-1, 27*2)\n","        logits = xenc @ W1  # predict logCounts\n","        counts = logits.exp()  # counts, equivalent to N\n","        p = counts / counts.sum(1, keepdims=True)  # probabilities for next character\n","        # ---------------\n","\n","        ix1 = ix2\n","        ix2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n","        out.append(itos[ix2])\n","        if ix2 == 0:\n","            break\n","    print(''.join(out))"]},{"cell_type":"markdown","metadata":{"id":"QKM0dJ8789Fv"},"source":["## E02: Split up the dataset randomly into 80% train set, 10% dev set, 10% test set.\n","Split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uKRO3TNTLDpI"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([156999, 2]) torch.Size([156999])\n","torch.Size([19452, 2]) torch.Size([19452])\n","torch.Size([19662, 2]) torch.Size([19662])\n"]}],"source":["# build the dataset\n","def build_dataset(words):\n","    X, Y = [], []\n","    for w in words:\n","      chs = ['.'] + list(w) + ['.']\n","      for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n","          ix1 = stoi[ch1]\n","          ix2 = stoi[ch2]\n","          ix3 = stoi[ch3]\n","\n","          X.append([ix1, ix2])\n","          Y.append(ix3)\n","\n","    X = torch.tensor(X)\n","    Y = torch.tensor(Y)\n","    print(X.shape, Y.shape)\n","    return X, Y\n","\n","import random\n","random.seed(42)\n","random.shuffle(words)\n","n1 = int(0.8*len(words))\n","n2 = int(0.9*len(words))\n","\n","Xtr, Ytr = build_dataset(words[:n1])\n","Xdev, Ydev = build_dataset(words[n1:n2])\n","Xte, Yte = build_dataset(words[n2:])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0AfCEDXxxk8I"},"outputs":[],"source":["# initialize the network\n","g = torch.Generator().manual_seed(2147483647)\n","W = torch.randn((27*2, 27), generator=g, requires_grad=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Q49nNrI5w8yg"},"outputs":[{"name":"stdout","output_type":"stream","text":["0: 4.1860\n","10: 2.5047\n","20: 2.3801\n","30: 2.3331\n","40: 2.3085\n","50: 2.2936\n","60: 2.2837\n","70: 2.2766\n","80: 2.2713\n","90: 2.2671\n","99: 2.2640\n"]}],"source":["# gradient descent\n","rep = 100\n","\n","for k in range(rep):\n","\n","  # forward pass\n","  xenc = F.one_hot(Xtr, num_classes=27).float()\n","  xenc = xenc.view(-1, 27*2) # this transforms the matrix from [N, 2, 27] -\u003e [N, 27*2] | (N, 27*2) @ (27*2, 27) -\u003e (N,   27)\n","\n","  # softmax\n","  logits = xenc @ W # log-counts\n","  counts = logits.exp() # equivalent N\n","  probs = counts / counts.sum(1, keepdims=True)\n","  # probs.shape = [4, 27]. Isso significa que, para cada \"trigram\" (que são 4 no total), ele output probabilidades\n","\n","  # loss\n","  loss = -probs[torch.arange(len(Xtr)), Ytr].log().mean()\n","\n","  if k % 10 == 0:\n","      print(f\"{k}: {loss.item():.4f}\")\n","\n","\n","  # backward pass\n","  W.grad = None # set to zero the gradient\n","  loss.backward()\n","\n","\n","  # update\n","  W.data += -50 * W.grad\n","\n","  if k == rep-1:\n","      print(f\"{k}: {loss.item():.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AReFQWTI06sH"},"outputs":[{"data":{"text/plain":["2.264030694961548"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["loss.item()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SxnhSZjkzXlo"},"outputs":[],"source":["def MLP_loss(x,y,W):\n","  xenc = F.one_hot(x, num_classes=27).float()\n","  xenc = xenc.view(-1, 27*2)\n","\n","  # softmax\n","  logits = xenc @ W\n","  counts = logits.exp()\n","  probs = counts / counts.sum(1, keepdim=True)\n","\n","  # Regularization / Model Smoothing\n","  reg = (W**2).mean()\n","  reg_str = 0.01\n","\n","  # loss\n","  loss = -probs[torch.arange(len(x)), y].log().mean() + reg_str * reg\n","\n","  return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qLa0v2l5wkWP"},"outputs":[{"name":"stdout","output_type":"stream","text":["Previous loss: 2.2727\n","Training loss: 2.2730\n","Dev loss: 2.2704\n","Testing loss: 2.2763\n"]}],"source":["# Evaluating the loss\n","\n","print(f'Previous loss: {MLP_loss(xs, ys, W1):.4f}')\n","print(f'Training loss: {MLP_loss(Xtr, Ytr, W):.4f}')\n","print(f'Dev loss: {MLP_loss(Xdev, Ydev, W):.4f}')\n","print(f'Testing loss: {MLP_loss(Xte, Yte, W):.4f}')"]},{"cell_type":"markdown","metadata":{"id":"e1jkSkNx9BYE"},"source":["## E03: Use the dev set\n","Use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"km-LTZ-31cUP"},"outputs":[],"source":["# initialize the network\n","g = torch.Generator().manual_seed(2147483647)\n","W = torch.randn((27*2, 27), generator=g, requires_grad=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4oDnTfwU4a6h"},"outputs":[],"source":["  # Regularization / Model Smoothing\n","  reg_str = -0.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"S8Wnuc-c1c4R"},"outputs":[{"name":"stdout","output_type":"stream","text":["0: Train Loss: 4.1860 | Dev Loss 4.2004\n","10: Train Loss: 2.5047 | Dev Loss 2.5072\n","20: Train Loss: 2.3801 | Dev Loss 2.3840\n","30: Train Loss: 2.3331 | Dev Loss 2.3382\n","40: Train Loss: 2.3085 | Dev Loss 2.3141\n","50: Train Loss: 2.2936 | Dev Loss 2.2995\n","60: Train Loss: 2.2837 | Dev Loss 2.2897\n","70: Train Loss: 2.2766 | Dev Loss 2.2827\n","80: Train Loss: 2.2713 | Dev Loss 2.2776\n","90: Train Loss: 2.2671 | Dev Loss 2.2736\n","100: Train Loss: 2.2637 | Dev Loss 2.2704\n","110: Train Loss: 2.2610 | Dev Loss 2.2679\n","120: Train Loss: 2.2587 | Dev Loss 2.2659\n","130: Train Loss: 2.2567 | Dev Loss 2.2641\n","140: Train Loss: 2.2551 | Dev Loss 2.2627\n","150: Train Loss: 2.2536 | Dev Loss 2.2615\n","160: Train Loss: 2.2524 | Dev Loss 2.2605\n","170: Train Loss: 2.2513 | Dev Loss 2.2596\n","180: Train Loss: 2.2503 | Dev Loss 2.2589\n","190: Train Loss: 2.2494 | Dev Loss 2.2582\n","199: Train Loss: 2.2487 | Dev Loss 2.2577\n"]}],"source":["# gradient descent\n","rep = 200\n","\n","for k in range(rep):\n","\n","  # forward pass\n","  xenc = F.one_hot(Xtr, num_classes=27).float()\n","  xenc = xenc.view(-1, 27*2) # this transforms the matrix from [N, 2, 27] -\u003e [N, 27*2] | (N, 27*2) @ (27*2, 27) -\u003e (N,   27)\n","\n","  # softmax\n","  logits = xenc @ W # log-counts\n","  counts = logits.exp() # equivalent N\n","  probs = counts / counts.sum(1, keepdims=True)\n","  # probs.shape = [4, 27]. Isso significa que, para cada \"trigram\" (que são 4 no total), ele output probabilidades\n","\n","  # Regularization / Model Smoothing\n","  reg = (W**2).mean()\n","\n","  # loss\n","  loss = -probs[torch.arange(len(Xtr)), Ytr].log().mean() + reg_str * reg\n","\n","  if k % 10 == 0:\n","      print(f\"{k}: Train Loss: {loss.item():.4f} | Dev Loss {MLP_loss(Xdev, Ydev, W):.4f}\")\n","\n","  # backward pass\n","  W.grad = None # set to zero the gradient\n","  loss.backward()\n","\n","\n","  # update\n","  W.data += -50 * W.grad\n","\n","  if k == rep-1:\n","      print(f\"{k}: Train Loss: {loss.item():.4f} | Dev Loss {MLP_loss(Xdev, Ydev, W):.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pRXA0BLk4Tzo"},"outputs":[],"source":["def MLP_loss(x,y,W):\n","  xenc = F.one_hot(x, num_classes=27).float()\n","  xenc = xenc.view(-1, 27*2)\n","\n","  # softmax\n","  logits = xenc @ W\n","  counts = logits.exp()\n","  probs = counts / counts.sum(1, keepdim=True)\n","\n","  # Regularization / Model Smoothing\n","  reg = (W**2).mean()\n","\n","  # loss\n","  loss = -probs[torch.arange(len(x)), y].log().mean() + reg_str * reg\n","\n","  return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MnQgV1Y65It-"},"outputs":[],"source":["# Data Tracking:\n","\n","# Why is a negative regularization strength reducing the loss always? A bit counterintuitive\n","# str = -0.10 | dev = 1.6252 | test = 1.6345\n","# str = -0.01 | dev = 2.2383 | test = 2.2447\n","# str = +0.00 | dev = 2.2511 | test = 2.2573\n","# str = +0.01 | dev = 2.2618\n","# str = +0.10 | dev = 2.3214\n","# str = +1.00 | dev = 2.5450"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0vVbTvVX4Vkn"},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing loss: 2.2530\n"]}],"source":["# Final evaluation of the loss\n","\n","print(f'Testing loss: {MLP_loss(Xte, Yte, W):.4f}')"]},{"cell_type":"markdown","metadata":{"id":"Q6ThQbxc9UOm"},"source":["#### Sampling"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gRWZvnhq9VXb"},"outputs":[{"name":"stdout","output_type":"stream","text":["aunide.\n","aliasad.\n","ushfay.\n","ainn.\n","aui.\n"]}],"source":["# finally, sample from the 'neural net' model\n","g = torch.Generator().manual_seed(2147483647)\n","\n","for i in range(5):\n","    out = []\n","    ix1, ix2 = 0, 0\n","    while True:\n","        # ---------------\n","        # # BEFORE:\n","        # # p = P[ix]\n","        # ---------------\n","        # NOW:\n","        xenc = F.one_hot(torch.tensor([ix1,ix2]), num_classes=27).float()\n","        xenc = xenc.view(-1, 27*2)\n","        logits = xenc @ W  # predict logCounts\n","        counts = logits.exp()  # counts, equivalent to N\n","        p = counts / counts.sum(1, keepdims=True)  # probabilities for next character\n","        # ---------------\n","\n","        ix1 = ix2\n","        ix2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n","        out.append(itos[ix2])\n","        if ix2 == 0:\n","            break\n","    print(''.join(out))"]},{"cell_type":"markdown","metadata":{"id":"qqIEd1zD9Sc5"},"source":["## E04: Rewrite the MLP model without creating one hot vectors\n","We saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Cv4gk3PledhC"},"outputs":[],"source":["# initialize the network\n","g = torch.Generator().manual_seed(2147483647)\n","W = torch.randn((27*2, 27), generator=g, requires_grad=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XUo22y0LUjLl"},"outputs":[{"name":"stdout","output_type":"stream","text":["199: Train Loss: 2.2487 | Dev Loss 2.2466\n"]}],"source":["# gradient descent\n","rep = 200\n","\n","for k in range(rep):\n","\n","  # forward pass\n","\n","\n","    # ---------------\n","    # # BEFORE:\n","    # logits = xenc @ W # log-counts\n","    # xenc = F.one_hot(Xtr, num_classes=27).float()\n","    # xenc = xenc.view(-1, 27*2) # this transforms the matrix from [N, 2, 27] -\u003e [N, 27*2] | (N, 27*2) @ (27*2, 27) -\u003e (N,   27)\n","    # ---------------\n","    # NOW:\n","\n","  # softmax\n","  logits = W[Xtr[:,0]] + W[Xtr[:,1] + 27]\n","  counts = logits.exp() # equivalent N\n","  probs = counts / counts.sum(1, keepdims=True)\n","  # probs.shape = [4, 27]. Isso significa que, para cada \"trigram\" (que são 4 no total), ele output probabilidades\n","\n","  # Regularization / Model Smoothing\n","  reg = (W**2).mean()\n","\n","  # loss\n","  loss = -probs[torch.arange(len(Xtr)), Ytr].log().mean() + reg_str * reg\n","\n","\n","  #if k % 10 == 0:\n","      #print(f\"{k}: Train Loss: {loss.item():.4f} | Dev Loss {MLP_loss(Xdev, Ydev, W):.4f}\")\n","\n","  # backward pass\n","  W.grad = None # set to zero the gradient\n","  loss.backward()\n","\n","\n","  # update\n","  W.data += -50 * W.grad\n","\n","  if k == rep-1:\n","      print(f\"{k}: Train Loss: {loss.item():.4f} | Dev Loss {MLP_loss(Xdev, Ydev, W):.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"HhOMBDeY9W3b"},"source":["## E05: Look up and use F.cross_entropy instead\n","Look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":522,"status":"ok","timestamp":1728061721008,"user":{"displayName":"Daiki","userId":"04202454946609493394"},"user_tz":180},"id":"PfBeuta4gSO2"},"outputs":[],"source":["# initialize the network\n","g = torch.Generator().manual_seed(2147483647)\n","W = torch.randn((27*2, 27), generator=g, requires_grad=True)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18138,"status":"ok","timestamp":1728061840290,"user":{"displayName":"Daiki","userId":"04202454946609493394"},"user_tz":180},"id":"Q1C8TDk4gSg3","outputId":"30750dc9-c3d8-4210-b239-da9983b9c977"},"outputs":[{"name":"stdout","output_type":"stream","text":["199: Train Loss: 2.2482 | Dev Loss 2.2487\n"]}],"source":["# gradient descent\n","rep = 200\n","\n","for k in range(rep):\n","\n","  # forward pass\n","\n","\n","    # ---------------\n","    # # BEFORE:\n","    # logits = xenc @ W # log-counts\n","    # xenc = F.one_hot(Xtr, num_classes=27).float()\n","    # xenc = xenc.view(-1, 27*2) # this transforms the matrix from [N, 2, 27] -\u003e [N, 27*2] | (N, 27*2) @ (27*2, 27) -\u003e (N,   27)\n","    # ---------------\n","    # NOW:\n","\n","  # softmax\n","  logits = W[Xtr[:,0]] + W[Xtr[:,1] + 27]\n","  # counts = logits.exp() # equivalent N\n","  # probs = counts / counts.sum(1, keepdims=True)\n","  # # probs.shape = [4, 27]. Isso significa que, para cada \"trigram\" (que são 4 no total), ele output probabilidades\n","\n","  # # Regularization / Model Smoothing\n","  # reg = (W**2).mean()\n","\n","  # # loss\n","  # loss = -probs[torch.arange(len(Xtr)), Ytr].log().mean() + reg_str * reg\n","\n","  loss = F.cross_entropy(logits, Ytr)\n","\n","\n","  #if k % 10 == 0:\n","      #print(f\"{k}: Train Loss: {loss.item():.4f} | Dev Loss {MLP_loss(Xdev, Ydev, W):.4f}\")\n","\n","  # backward pass\n","  W.grad = None # set to zero the gradient\n","  loss.backward()\n","\n","\n","  # update\n","  W.data += -50 * W.grad\n","\n","  if k == rep-1:\n","      print(f\"{k}: Train Loss: {loss.item():.4f} | Dev Loss {MLP_loss(Xdev, Ydev, W):.4f}\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMI46gjumQmybbZLXeHweLT","mount_file_id":"1AKpe4YqKYHlMd1WK_QlHh-UxNdyoMZki","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
